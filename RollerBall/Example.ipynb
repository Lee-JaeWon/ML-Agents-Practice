{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8290b572",
   "metadata": {},
   "source": [
    "## Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da84826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import mlagents\n",
    "  print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "  !python -m pip install -q mlagents==0.27.0\n",
    "  print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959167a",
   "metadata": {},
   "source": [
    "### Unity환경을 build한 파일 위치(.exe가 포함된)설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e9c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"E:/RL_Algorithm/RollerBall/ML_test/ML_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83dfccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=env_id, seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf7434",
   "metadata": {},
   "source": [
    "### Environment Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7d7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81c7c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the behavior : RollerBall?team=0\n"
     ]
    }
   ],
   "source": [
    "# We will only consider the first Behavior\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "print(f\"Name of the behavior : {behavior_name}\")\n",
    "spec = env.behavior_specs[behavior_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6c9bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  1\n",
      "Is there a visual observation ? False\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(spec.observation_specs))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "# Visual observation have 3 dimensions: Height, Width and number of channels\n",
    "vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "print(\"Is there a visual observation ?\", vis_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01483faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 continuous actions\n",
      "Action number 0 has 1 different options\n"
     ]
    }
   ],
   "source": [
    "# Is the Action continuous or multi-discrete ?\n",
    "if spec.action_spec.continuous_size > 0:\n",
    "  print(f\"There are {spec.action_spec.continuous_size} continuous actions\")\n",
    "if spec.action_spec.is_discrete():\n",
    "  print(f\"There are {spec.action_spec.discrete_size} discrete actions\")\n",
    "\n",
    "\n",
    "# How many actions are possible ?\n",
    "#print(f\"There are {spec.action_size} action(s)\")\n",
    "\n",
    "# For discrete actions only : How many different options does each action has ?\n",
    "if spec.action_spec.discrete_size > 0:\n",
    "  for action, branch_size in enumerate(spec.action_spec.discrete_branches):\n",
    "    print(f\"Action number {action} has {branch_size} different options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894ccfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e0248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d01ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a594ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vector observations :  [3.9974775 0.5       2.1941023 0.        0.5       0.        0.\n",
      " 0.       ]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 3:\n",
    "    print(\"Here is the first visual observation\")\n",
    "    plt.imshow(decision_steps.obs[index][0,:,:,:])\n",
    "    plt.show()\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 1:\n",
    "    print(\"First vector observations : \", decision_steps.obs[index][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4c484",
   "metadata": {},
   "source": [
    "## Run the Environment for a few episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26529f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards for episode 0 is 1.0\n",
      "Total rewards for episode 1 is 1.0\n",
      "Total rewards for episode 2 is 0.0\n",
      "Total rewards for episode 3 is 0.0\n",
      "Total rewards for episode 4 is 0.0\n",
      "Total rewards for episode 5 is 0.0\n",
      "Total rewards for episode 6 is 0.0\n",
      "Total rewards for episode 7 is 0.0\n",
      "Total rewards for episode 8 is 1.0\n",
      "Total rewards for episode 9 is 0.0\n",
      "Total rewards for episode 10 is 1.0\n",
      "Total rewards for episode 11 is 0.0\n",
      "Total rewards for episode 12 is 0.0\n",
      "Total rewards for episode 13 is 1.0\n",
      "Total rewards for episode 14 is 0.0\n",
      "Total rewards for episode 15 is 1.0\n",
      "Total rewards for episode 16 is 0.0\n",
      "Total rewards for episode 17 is 0.0\n",
      "Total rewards for episode 18 is 1.0\n",
      "Total rewards for episode 19 is 0.0\n",
      "Total rewards for episode 20 is 0.0\n",
      "Total rewards for episode 21 is 0.0\n",
      "Total rewards for episode 22 is 0.0\n",
      "Total rewards for episode 23 is 0.0\n",
      "Total rewards for episode 24 is 0.0\n",
      "Total rewards for episode 25 is 0.0\n",
      "Total rewards for episode 26 is 0.0\n",
      "Total rewards for episode 27 is 0.0\n",
      "Total rewards for episode 28 is 1.0\n",
      "Total rewards for episode 29 is 0.0\n",
      "Total rewards for episode 30 is 0.0\n",
      "Total rewards for episode 31 is 0.0\n",
      "Total rewards for episode 32 is 0.0\n",
      "Total rewards for episode 33 is 0.0\n",
      "Total rewards for episode 34 is 0.0\n",
      "Total rewards for episode 35 is 0.0\n",
      "Total rewards for episode 36 is 0.0\n",
      "Total rewards for episode 37 is 0.0\n",
      "Total rewards for episode 38 is 0.0\n",
      "Total rewards for episode 39 is 0.0\n",
      "Total rewards for episode 40 is 0.0\n",
      "Total rewards for episode 41 is 0.0\n",
      "Total rewards for episode 42 is 0.0\n",
      "Total rewards for episode 43 is 1.0\n",
      "Total rewards for episode 44 is 1.0\n",
      "Total rewards for episode 45 is 0.0\n",
      "Total rewards for episode 46 is 0.0\n",
      "Total rewards for episode 47 is 0.0\n",
      "Total rewards for episode 48 is 0.0\n",
      "Total rewards for episode 49 is 0.0\n",
      "Total rewards for episode 50 is 0.0\n",
      "Total rewards for episode 51 is 0.0\n",
      "Total rewards for episode 52 is 1.0\n",
      "Total rewards for episode 53 is 0.0\n",
      "Total rewards for episode 54 is 0.0\n",
      "Total rewards for episode 55 is 0.0\n",
      "Total rewards for episode 56 is 0.0\n",
      "Total rewards for episode 57 is 0.0\n",
      "Total rewards for episode 58 is 1.0\n",
      "Total rewards for episode 59 is 0.0\n",
      "Total rewards for episode 60 is 0.0\n",
      "Total rewards for episode 61 is 1.0\n",
      "Total rewards for episode 62 is 0.0\n",
      "Total rewards for episode 63 is 0.0\n",
      "Total rewards for episode 64 is 0.0\n",
      "Total rewards for episode 65 is 0.0\n",
      "Total rewards for episode 66 is 0.0\n",
      "Total rewards for episode 67 is 0.0\n",
      "Total rewards for episode 68 is 1.0\n",
      "Total rewards for episode 69 is 1.0\n",
      "Total rewards for episode 70 is 0.0\n",
      "Total rewards for episode 71 is 1.0\n",
      "Total rewards for episode 72 is 0.0\n",
      "Total rewards for episode 73 is 1.0\n",
      "Total rewards for episode 74 is 0.0\n",
      "Total rewards for episode 75 is 0.0\n",
      "Total rewards for episode 76 is 0.0\n",
      "Total rewards for episode 77 is 1.0\n",
      "Total rewards for episode 78 is 0.0\n",
      "Total rewards for episode 79 is 0.0\n",
      "Total rewards for episode 80 is 0.0\n",
      "Total rewards for episode 81 is 0.0\n",
      "Total rewards for episode 82 is 0.0\n",
      "Total rewards for episode 83 is 0.0\n",
      "Total rewards for episode 84 is 0.0\n",
      "Total rewards for episode 85 is 0.0\n",
      "Total rewards for episode 86 is 0.0\n",
      "Total rewards for episode 87 is 1.0\n",
      "Total rewards for episode 88 is 0.0\n",
      "Total rewards for episode 89 is 0.0\n",
      "Total rewards for episode 90 is 0.0\n",
      "Total rewards for episode 91 is 0.0\n",
      "Total rewards for episode 92 is 0.0\n",
      "Total rewards for episode 93 is 0.0\n",
      "Total rewards for episode 94 is 0.0\n",
      "Total rewards for episode 95 is 1.0\n",
      "Total rewards for episode 96 is 1.0\n",
      "Total rewards for episode 97 is 1.0\n",
      "Total rewards for episode 98 is 0.0\n",
      "Total rewards for episode 99 is 0.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "  env.reset()\n",
    "  decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "  tracked_agent = -1 # -1 indicates not yet tracking\n",
    "  done = False # For the tracked_agent\n",
    "  episode_rewards = 0 # For the tracked_agent\n",
    "  while not done:\n",
    "    # Track the first agent we see if not tracking\n",
    "    # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "      tracked_agent = decision_steps.agent_id[0]\n",
    "\n",
    "    # Generate an action for all agents\n",
    "    action = spec.action_spec.random_action(len(decision_steps))\n",
    "\n",
    "    # Set the actions\n",
    "    env.set_actions(behavior_name, action)\n",
    "\n",
    "    # Move the simulation forward\n",
    "    env.step()\n",
    "\n",
    "    # Get the new simulation results\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    if tracked_agent in decision_steps: # The agent requested a decision\n",
    "      episode_rewards += decision_steps[tracked_agent].reward\n",
    "    if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "      episode_rewards += terminal_steps[tracked_agent].reward\n",
    "      done = True\n",
    "  print(f\"Total rewards for episode {episode} is {episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4f903",
   "metadata": {},
   "source": [
    "## Close the Environment to free the port it is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5b7ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
